<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Taewoo Kim - Computer Vision Researcher</title>
<meta name="viewport" content="width=device-width, initial-scale=1">


<style>
    /* TOP MAIN HEADLINE */
    .top-headline {
        width: 100%;
        text-align: left;
        font-size: 30px;
        font-weight: 700;
        font-family: Georgia, serif;
        padding: 28px 0 8px 0;
        color: #0A3D62;   /* Í≥†Í∏âÏä§Îü¨Ïö¥ Îî•Î∏îÎ£® */
        letter-spacing: 0.5px;
    }

    /* Blue underline (matches container width) */
    .top-line {
        width: 930px;                 /* ÏßßÍ≥† ÏÑ∏Î†®Îêú ÎùºÏù∏ Í∏∏Ïù¥ */
        height: 4px;
        background-color: #007acc;
        margin: 6px 0 35px 0;         /* ÏôºÏ™Ω Ï†ïÎ†¨ÏùÑ ÏúÑÌï¥ auto Ï†úÍ±∞ */
        border-radius: 3px;
    }


  /* GLOBAL */
  body {
      font-family: Georgia, "Times New Roman", serif;
      margin: 0;
      padding: 0;
      background: #ffffff;
      color: #1a1a1a;
      line-height: 1.65;
      font-size: 17px;   /* üî• Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ ÌÅ¨Í∏∞ ÏÑ§Ï†ï */
  }

  a {
      color: #007acc;
      text-decoration: none;
  }
  a:hover { text-decoration: underline; }

  /* WRAPPER */
  .container {
      max-width: 930px;
      margin: auto;
      padding: 50px 48px 70px;  /* Ï¢åÏö∞ spacing Í∑†ÌòïÏ†Å */
  }

  /* HEADER */
  .header {
      display: flex;
      align-items: center;
      gap: 25px;
      margin-bottom: 35px;
  }
  .header img {
      width: 160px;
      height: 160px;
      border-radius: 4px;
      object-fit: cover;
  }
  .name {
      font-size: 36px;
      font-weight: 700;
      margin-bottom: 6px;
      font-family: Georgia, serif;
  }
  .position { font-size: 17px; color: #333; }

  /* TITLES */
  h2 {
      margin-top: 45px;
      margin-bottom: 12px;
      padding-bottom: 6px;
      border-bottom: 1px solid #ddd;
      font-size: 24px;
      font-weight: 700;
      font-family: Georgia, serif;
  }

  /* PUBLICATIONS */
  .pub-item {
      padding: 7px 0;
      margin-bottom: 7px;
      border-bottom: none; 
  }
  .year {
      font-weight: bold;
      color: #555;
      font-size: 16px;
  }
  .award { color: #b30000; font-weight: bold; }

  /* EDUCATION */
  .edu-block { margin-bottom: 20px; }
  .edu-title {
      font-weight: bold;
      display: flex;
      justify-content: space-between;
  }
  .edu-sub {
      color: #555;
      font-size: 15px;
      font-style: italic;
  }

  /* FOOTER */
  .footer {
      text-align: center;
      margin-top: 60px;
      font-size: 14px;
      color: #888;
  }
</style>
</head>


<body>
<div class="container">


<!-- TOP MAIN HEADLINE -->
<div class="top-headline">
    Taewoo Kim ‚Äî Portfolio
</div>
<div class="top-line"></div>

<!-- HEADER -->
<div class="header">
  <img src="profile.png" alt="Profile Photo">
  <div>
      <div class="position">Taewoo Kim, Ph.D. (KAIST)</div>
      <div class="position">Machine Learning Engineer, Qualcomm Korea</div>
      <div>Email: an625148 (at) gmail.com</div>
      <div style="margin-top: 8px;">
        <a href="https://www.linkedin.com/in/taewookim-a85270168/">LinkedIn</a> ¬∑
        <a href="https://scholar.google.com/citations?user=SzKw5oYAAAAJ&hl=en">Google Scholar</a> ¬∑
        <a href="https://github.com/intelpro">GitHub</a>
      </div>
  </div>
</div>




<!-- ABOUT -->
<h2>About me</h2>
<p>
I am a <b>Machine learning Engineer</b> at Qualcomm Korea, developing ML-based computational imaging and camera algorithms.
Before joining Qualcomm, I completed my Ph.D. at KAIST under the supervision of 
<b>Prof. Kuk-Jin Yoon</b>, where I worked on <b>computer vision</b>, <b>machine learning</b>, and 
computational imaging. 
My recent research interests include <b>Generative AI</b>, 
<b>computational photography</b>, and <b>multi-modal data fusion</b>.
</p>


<!-- PUBLICATIONS -->
<h2>Publications</h2>

<div class="pub-item">
  <span class="year">ICCV 2025</span><br>
  <b>Event-guided Unified Framework for Low-light Video Enhancement, Frame Interpolation, and Deblurring</b><br>
  <b>Taewoo Kim</b>, Kuk-Jin Yoon<br>
  <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Kim_Event-guided_Unified_Framework_for_Low-light_Video_Enhancement_Frame_Interpolation_and_ICCV_2025_paper.pdf">
    <span style="color:black;">[</span><span style="color:#007acc;">Paper</span><span style="color:black;">]</span>
  </a>
</div>

<div class="pub-item">
  <span class="year">NeurIPS 2024</span><br>
  <b>A Benchmark Dataset for Event-Guided Human Pose Estimation and Tracking in Extreme Conditions</b><br>
  Hoonhee Cho*, <b>Taewoo Kim*</b>, Yuhwan Jeong, Kuk-Jin Yoon<br>
  <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/f304e427cfe6bb762fe1bf18516c8a87-Paper-Datasets_and_Benchmarks_Track.pdf">
    <span style="color:black;">[</span><span style="color:#007acc;">Paper</span><span style="color:black;">]</span>
  </a>
</div>

<div class="pub-item">
  <span class="year">ECCV 2024</span><br>
  <b>Towards Real-world Event-guided Low-light Video Enhancement and Deblurring</b><br>
  <b>Taewoo Kim</b>, Jaeseok Jeong, Hoonhee Cho, Yuhwan Jeong, Kuk-Jin Yoon<br>
  <a href="https://arxiv.org/pdf/2408.14916">
    <span style="color:black;">[</span><span style="color:#007acc;">Paper</span><span style="color:black;">]</span>
  </a>
</div>

<div class="pub-item">
  <span class="year">ECCV 2024</span><br>
  <b>CMTA: Cross-Modal Temporal Alignment for Event-guided Video Deblurring</b><br>
  <b>Taewoo Kim*</b>, Hoonhee Cho*, Kuk-Jin Yoon<br>
  <a href="https://arxiv.org/pdf/2408.14930">
    <span style="color:black;">[</span><span style="color:#007acc;">Paper</span><span style="color:black;">]</span>
  </a>
</div>

<div class="pub-item">
  <span class="year">CVPR 2024</span><br>
  <b>Frequency-aware Event-based Video Deblurring for Real-world Motion Blur</b><br>
  <b>Taewoo Kim*</b>, Hoonhee Cho*, Kuk-Jin Yoon<br>
  <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Frequency-aware_Event-based_Video_Deblurring_for_Real-World_Motion_Blur_CVPR_2024_paper.pdf">
    <span style="color:black;">[</span><span style="color:#007acc;">Paper</span><span style="color:black;">]</span>
  </a>
</div>

<div class="pub-item">
  <span class="year">CVPR 2024</span><br>
  <b>TTA-EVF: Test-Time Adaptation for Event-based Video Frame Interpolation via Reliable Pixel and Sample Estimation</b><br>
  Hoonhee Cho, <b>Taewoo Kim</b>, Yuhwan Jeong, Kuk-Jin Yoon<br>
  <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cho_TTA-EVF_Test-Time_Adaptation_for_Event-based_Video_Frame_Interpolation_via_Reliable_CVPR_2024_paper.pdf">
    <span style="color:black;">[</span><span style="color:#007acc;">Paper</span><span style="color:black;">]</span>
  </a>
</div>

<div class="pub-item">
  <span class="year">ICCV 2023</span><br>
  <b>Non-coaxial Event-Guided Motion Deblurring with Spatial Alignment</b><br>
  Hoonhee Cho, Yuhwan Jeong, <b>Taewoo Kim</b>, Kuk-Jin Yoon<br>
  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_Non-Coaxial_Event-Guided_Motion_Deblurring_with_Spatial_Alignment_ICCV_2023_paper.pdf">
    <span style="color:black;">[</span><span style="color:#007acc;">Paper</span><span style="color:black;">]</span>
  </a>
</div>

<div class="pub-item">
  <span class="year">CVPR 2023</span><br>
  <b>Event-based Video Frame Interpolation with Cross-Modal Asymmetric Bidirectional Motion Fields</b><br>
  <b>Taewoo Kim</b>, Yujeong Chae, Hyun-Kurl Jang, Kuk-Jin Yoon<br>
  <span class="award">Highlight Paper (Top 2.5%)</span><br>
  <a href="https://arxiv.org/pdf/2502.13716">
    <span style="color:black;">[</span><span style="color:#007acc;">Paper</span><span style="color:black;">]</span>
  </a>
</div>

<div class="pub-item">
  <span class="year">ECCV 2022</span><br>
  <b>Event-guided Deblurring of Unknown Exposure Time Videos</b><br>
  <b>Taewoo Kim</b>, Jeongmin Lee, Wang Lin, Kuk-Jin Yoon<br>
  <span class="award">Oral Presentation (Top 2.7%)</span><br>
  <a href="https://arxiv.org/abs/2112.06988">
    <span style="color:black;">[</span><span style="color:#007acc;">Paper</span><span style="color:black;">]</span>
  </a>
</div>

<div class="pub-item">
  <span class="year">RA-L + IROS 2020</span><br>
  <b>Loop-Net: Joint Unsupervised Disparity and Optical Flow Estimation of Stereo Videos with Spatiotemporal Loop Consistency</b><br>
  <b>Taewoo Kim</b>, Kwonyoung Ryu, Kyeongseob Song, Kuk-Jin Yoon<br>
  <span class="award">Oral Presentation</span><br>
  <a href="https://ieeexplore.ieee.org/document/9140315">
    <span style="color:black;">[</span><span style="color:#007acc;">Paper</span><span style="color:black;">]</span>
  </a>
</div>


<!-- EDUCATION -->
<h2>Education</h2>

<div class="edu-block">
  <div class="edu-title">
    <span>Ph.D. in Mechanical Engineering, KAIST</span>
    <span>Feb. 2025</span>
  </div>
  <div class="edu-sub">Thesis: Video Enhancement with Event Cameras</div>
  <div class="edu-sub">Advisor: Prof. Kuk-Jin Yoon</div>
</div>

<div class="edu-block">
  <div class="edu-title">
    <span>M.S. in Robotics, KAIST</span>
    <span>Feb. 2020</span>
  </div>
  <div class="edu-sub">Thesis: Joint Unsupervised Disparity and Optical Flow Estimation</div>
  <div class="edu-sub">Advisor: Prof. Kuk-Jin Yoon</div>
</div>

<div class="edu-block">
  <div class="edu-title">
    <span>B.S. in Mechanical Engineering, Yonsei University</span>
    <span>Feb. 2017</span>
  </div>
</div>



<!-- AWARDS -->
<h2>Awards</h2>
<ul style="list-style-type: none; padding-left: 0; margin-top: 10px;">
  <li><b>CVPR 2023 Highlight Paper</b> ‚Äî Top 2.5% of all submissions</li>
  <li><b>ECCV 2022 Oral Presentation</b> ‚Äî Top 2.7% of all submissions</li>
  <li><b>IPIU 2023 Gold Prize</b> ‚Äî Second Best Paper Award</li>
</ul>

<!-- REVIEWER SERVICES -->
<h2>Reviewer Services</h2>
<ul style="list-style-type: none; padding-left: 0; margin-top: 10px;">
  <li><b>Conferences:</b> CVPR, ICCV, ECCV, WACV, NeurIPS, SIGGRAPH Asia</li>
  <li><b>Journals:</b> IEEE TPAMI, IJCV, Neural Networks, RA-L, IJAT</li>
</ul>



<!-- CONTACT -->
<h2>Contact</h2>
Email: an625148 (at) gmail.com


<div class="footer">¬© 2025 Taewoo Kim</div>

</div>
</body>
</html>